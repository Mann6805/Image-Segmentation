{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e868cc8d-4c21-45e7-ba5d-9a5ca843491a",
   "metadata": {},
   "source": [
    "### Model U-net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2862ad1d-3f03-49a1-9b13-9db133752e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623002f2-11c7-4328-872e-4f20cb3a41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3,1,1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3,1,1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e960bd0-82b7-4db9-9186-74e702099ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512],):\n",
    "        super(UNET,self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "        # Down Part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature #pass to next level\n",
    "    \n",
    "        # Up Part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2,feature))\n",
    "    \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "    \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "    \n",
    "        x = self.bottleneck(x)\n",
    "    \n",
    "        skip_connections = skip_connections[::-1] #reverse the connection\n",
    "            \n",
    "        for idx in range(0,len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if(x.shape != skip_connection.shape):\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "                \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "                \n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0fb46e-1a75-459d-9d06-e3e13c5e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    x = torch.randn((3,1,160,160))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    print(x.shape)\n",
    "    assert preds.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794f9fa3-48a7-4bcb-9cf8-e74bfd343391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 160, 160])\n",
      "torch.Size([3, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4bffa-4e4d-45c7-bc60-d37981d7076b",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e766fdb-8d13-47d2-ab93-122dca1ab0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93816c3f-15ee-47d1-9053-e4cf8db785da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class Carvana(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and mask\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        \n",
    "        # Open image and mask as PIL images\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
    "\n",
    "        # Convert mask to numpy array and set appropriate values (0 for background, 1 for object)\n",
    "        mask = np.array(mask, dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        # Apply resize transformation if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = Image.fromarray(mask)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Convert mask to tensor\n",
    "        mask = np.array(mask, dtype=np.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7f7d9-9376-4d9f-aa73-ffaf7e567a42",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdc8097-4747-4664-aa37-031b71b125a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c284f838-4355-4978-8074-b6b8083a09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 20\n",
    "shuffle=True\n",
    "pin_memory=True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_DIR = \"data/train\"\n",
    "MASK_DIR = \"data/train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268c416f-f9a3-418b-9084-1022202d885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2924241b-eb86-4002-95d3-e41f4efaa47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return 1 - (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "train_dataset = Carvana(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    mask_dir=MASK_DIR,\n",
    "    transform=transform  # Add augmentation logic if needed\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e50b8a3-ce5e-4531-9c2f-4abb01588cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d3c4f75-733a-4b18-8b1a-0d9967ec0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(loop):\n",
    "        print(f\"Batch {batch_idx + 1}/{len(train_loader)}\")  # Log batch progress\n",
    "\n",
    "        # Move data to the appropriate device\n",
    "        images = images.to(DEVICE, dtype=torch.float32)\n",
    "        masks = masks.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # Check if images and masks are valid\n",
    "        if images.size(0) == 0 or masks.size(0) == 0:\n",
    "            print(f\"Skipping empty batch {batch_idx + 1}\")\n",
    "            continue  # Skip this batch if it's empty\n",
    "\n",
    "        # Ensure masks have the correct shape\n",
    "        masks = masks.unsqueeze(1)  # Add a channel dimension if needed\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Check if predictions and masks have the correct shape\n",
    "        if predictions.size(1) != masks.size(1):  # Ensure they match\n",
    "            print(f\"Shape mismatch: predictions {predictions.size()}, masks {masks.size()}\")\n",
    "            continue  # Skip this batch if there's a shape mismatch\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(predictions, masks) + dice_loss(predictions, masks)\n",
    "\n",
    "        # Add the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Clear batch data\n",
    "        del images, masks, predictions, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763856fa-0c49-4904-ad64-5f76bb5430e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1272 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}]\")\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
